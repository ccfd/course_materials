\documentclass{instrukcja}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[OT4]{fontenc}

%\usepackage{dsfont}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%\usepackage[justification=centering]{caption}

\begin{document}

\materialnumber{1}
\course[MetNum]{Metody Numeryczne}
\material[Lab 1]{Instrukcja 1}
\author{B. Górecki}
%Układy równań nieliniowych
\materialtitle
%\title{Metody iteracyjne}

Na tych laboratoriach skupimy się na rozwiązaniu układu:
\[Ax = b\]
Naszym celem będzie więc napisanie funkcji {\tt Solve} zastępującej funkcję {\tt Gauss}. Nie będziemy jednak tego układu rozwiązywać metodą bezpośrednią, taką jak eliminacja Gaussa, ale metodą iteracyjną. Tzn: będziemy konstruować kolejne przybliżenia $x^{(n)}$ dokładnego $x$, takie że $b-Ax^{(n)}$ będzie coraz bliższe zeru.

 $r = b-Ax^{(n)}$ nazywamy {\bf residual'em}.

\begin{zad}Policz residual. Następnie policz i wyświetl jego normę: $\|r\|=\sqrt{r^Tr}$ (napisz funkcję liczącą normę wektora {\tt norm(double *,int)}). Ile wynosi ta norma przed i po rozwiązaniu układu metodą eliminacji Gaussa?\end{zad}

\section{Na głupa}

Pierwszym pomysłem na iteracyjne rozwiązywanie byłoby postawienie:
\[x^{(n+1)} = x^{(n)} + p\]
Gdzie $p$ jest ,,poprawką'' w iteracji. Łatwo sprawdzić, że idealne $p$ byłoby równe:
\[p = A^{-1}r\]
Jednak nie mamy $A^{-1}$ (w tym rzecz). Zamiast niej użyjemy $M^{-1}$, gdzie $M$ będzie przybliżeniem $A$. Macierz $M^{-1}$ nazywamy {\bf preconditioner'em}. Na początek zamiast rozwiązywać pełen układ, pominiemy większość jego elementów:
\[\begin{array}{rrrrrl}
A_{11}p_1 & \gray +A_{12}p_2&\gray +A_{13}p_1&\gray +\cdots &\gray +A_{1n}p_n&= r_1\\
\gray A_{21}p_1&+ A_{22}p_2 &\gray +A_{23}p_1&\gray +\cdots &\gray +A_{2n}p_n&= r_2\\
\gray A_{31}p_1&\gray +A_{32}p_2&+A_{33}p_1&\gray +\cdots &\gray +A_{3n}p_n&= r_3\\
\cdots\\
\gray A_{n1}p_1&\gray +A_{n2}p_2&\gray +A_{n3}p_1&\gray +\cdots &+A_{nn}p_n&= r_n\\
\end{array}\]
Co daje nam prosty wzór na $p$:
\[p_i = \frac{1}{A_{ii}}{r_i}\]
Jest to równoważne z wzięciem za $M$ diagonalnej części $A$. Ten prosty schemat iteracji, z powyższą poprawką nazywamy {\bf metodą Jacobiego}.

\begin{zad} Zaczynając od $x=0$ powtarzaj tą prostą iterację (np. 1000 razy). W każdej iteracji wyświetlaj normę residualu, a także wywołaj funkcję {\tt draw\_residual(double)} by wykonać wykres zbieżności.\end{zad}

Tak wykonana iteracja się nie zbiega. Wprowadźmy współczynnik, który ,,przytłumi'' wykonywane iteracje:
\[x^{(n+1)} = x^{(n)} + \alpha p\]
\begin{zad} Sprawdź zbieżność tego schematu przy różnych $\alpha$. Sprawdź $0.5$, $0.9$, $1.1$ i $2$.\end{zad}
\begin{zad} Wydziel z funkcji {\tt Solve} część odpowiedzialną za mnożenie przez $A$: {\tt Mult(double** A, double*x, double* r)} i preconditioner: {\tt Precond(double** A, double*x, double* p)}\end{zad}

Spróbujmy poprawić nasz schemat biorąc lepszy preconditioner. Zauważmy, że licząc $p_2$ mamy już obliczone $p_1$ i możemy go użyć. Tak więc nie musimy pomijać elementów układu ,,pod diagonalą'':
\[\begin{array}{rrrrrl}
A_{11}p_1 & \gray +A_{12}p_2&\gray +A_{13}p_1&\gray +\cdots &\gray +A_{1n}p_n&= r_1\\
A_{21}p_1&+ A_{22}p_2 &\gray +A_{23}p_1&\gray +\cdots &\gray +A_{2n}p_n&= r_2\\
A_{31}p_1& +A_{32}p_2&+A_{33}p_1&\gray +\cdots &\gray +A_{3n}p_n&= r_3\\
\cdots\\
A_{n1}p_1& +A_{n2}p_2& +A_{n3}p_1& +\cdots &+A_{nn}p_n&= r_n\\
\end{array}\]
Co daje nam prosty wzór na $p$:
\[p_i = \frac{1}{A_{ii}}(r_i - \sum_{j=1}^{i-1}A_{ij}p_j)\]
Gdy $\alpha=1$ schemat taki nazywamy {\bf Metodą Gaussa-Seidla}. 

\begin{zad} Wypróbuj nowy wzór na $p$, znów sprawdzając różne $\alpha$.\end{zad}

Schematy z $\alpha > 1$ nazywamy metodami {\bf Successive Over-Relaxation} (SOR).

\section{Dobieramy $\alpha$}
Widać wyraźnie, że zbieżność bardzo zależy od $\alpha$ i jasnym jest, że najlepiej byłoby dobierać ten współczynnik w każdej iteracji. Zauważmy że residual po iteracji wynosi:
\[\hat{r} = r - \alpha Ap\]
Spróbujmy zminimalizować kwadrat normy tego residualu:
\[\hat{r}^T\hat{r} = (r - \alpha Ap)^T(r - \alpha Ap) = r^Tr-2\alpha r^TAp+\alpha^2(Ap)^TAp\]
Licząc pochodną po $\alpha$ mamy:
\[-r^TAp+2\alpha(Ap)^TAp=0\]
Ostatecznie:
\[\alpha = \frac{r^TAp}{(Ap)^TAp}\]
Schemat z takim $\alpha$ nazywamy metodą {\bf MINRES}.

\begin{zad} Oblicz wektor $Ap$. Zauważ, że wyrażenie $a^Tb$ to iloczyn skalarny dwóch wektorów $a^Tb = a\cdot b$. Napisz funkcję liczącą iloczyn skalarny {\tt skal(double*, double*, int)} i oblicz $\alpha$ z powyższego wzoru. Sprawdź zbieżność przy takim $\alpha$.\end{zad}

\section{Wycinamy nadmiary}
Przez $q$ oznaczmy poprawkę z poprzedniej iteracji. Można powiedzieć, że w następnej iteracji nie chcemy ,,stracić'' tego co ,,zyskaliśmy'' w poprzedniej. Dlatego za nową poprawkę weźmiemy $p - \beta q$. Teraz wzór na nowy residual będzie:
\[\hat{r} = r - \alpha A(p-\beta q)\]
\begin{zad} Wypisz wzór na $\hat{r}^T\hat{r}$ i zróżniczkuj go po $\beta$. Wylicz $\beta$ przyjmując, że $r^T Aq = 0$ (to wynika z poprzedniej iteracji).\end{zad}

\begin{zad} Zmodyfikuj iterację wg. schematu:
\begin{itemize}
\item oblicz residual
\item oblicz $p=M^{-1}r$
\item jeżeli to nie pierwsza iteracja: oblicz $\beta$ i nową poprawkę: $p=p-\beta q$
\item oblicz $\alpha$
\item wylicz nowe rozwiązanie $x=x+\alpha p$
\item zachowaj poprawkę $q=p$ (opłaca się też zachować $Ap$)
\end{itemize}
\end{zad}

\section{A jeśli $A$ jest symetryczna i dodatnio określona ...}
W naszym przypadku możemy wykorzystać fakt, że macierz $A$ jest symetryczna i dodatnio określona. Wtedy zamiast minimalizować $r^Tr$ możemy minimalizować pewien specjalny funkcjonał:
\[\frac{1}{2}x^TAx - b^Tx\]
{\bf Pytanie:} Jakie fizyczne wyjaśnienie mają następujące rzeczy w naszym przypadku:
\begin{itemize}
\item Czym jest powyższy funkcjonał?
\item Dlaczego $A$ jest symetryczna?
\item Dlaczego $A$ jest dodatnio określona?
\end{itemize}

\begin{zad} Podstaw w powyższym wzorze $x = x^{(n)} + \alpha p$, zróżniczkuj i wylicz $\alpha$. Zauważ, że $\frac{1}{2}x^TAx - b^Tx = \text{const} + \frac{1}{2}(\alpha p)^TA(\alpha p) - r^T(\alpha p)$.\end{zad}

\begin{zad} Analogicznie jak poprzednio, podstaw $x = x^{(n)} + \alpha (p-\beta q)$, zróżniczkuj i wylicz $\beta$.(tym razem $q^Tr = 0$)\end{zad}

\begin{zad} Zastosuj dokładnie identyczną iterację zamieniając jedynie $\alpha$ i $\beta$ i zbadaj zbieżność.\end{zad}

Schemat taki nazywamy metodą {\bf gradientu sprzężonego} --- Conjugate Gradient Method ({\bf CG}).

{\bf Uwaga:} Aktualnie zbieżność jest bardzo słaba. Wynika to z faktu, że choć $A$ jest symetryczna to preconditioner z metody Gaussa-Seidla $M^{-1}$ już nie jest.

\begin{zad} Zbadaj zbieżność z preconditionerem diagonalnym, lub wyrażeniem $p=r$ (brakiem preconditionera).\end{zad}

{\bf Uwaga:} Metodę Conjugate Gradient można zaimplementować w bardziej ,,zwartej'' formie. Taki schemat można znaleść na wikipedii, bądz w notatkach z wykładu.
\end{document}
